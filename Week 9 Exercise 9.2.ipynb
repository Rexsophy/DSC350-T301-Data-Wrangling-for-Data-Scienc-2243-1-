{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84e161ce-f536-4a03-b3f0-e4f4cfece842",
   "metadata": {},
   "source": [
    "## Assignment 9.2\n",
    "Author: Rex Gayas\n",
    "Date: 11 February 2024\n",
    "Modified By: N/A\n",
    "Description: Exploring data extraction and manipulation and applying different data storage formats for efficient data handling and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8f2a6a5-b2fb-41d1-aa82-b6325ec2de64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3\n",
      "0  0.548814  0.715189  0.602763  0.544883\n",
      "1  0.423655  0.645894  0.437587  0.891773\n",
      "2  0.963663  0.383442  0.791725  0.528895\n"
     ]
    }
   ],
   "source": [
    "# Task 1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Seed the random generator \n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate a 3x4 NumPy array with random values\n",
    "random_array = np.random.rand(3, 4)\n",
    "\n",
    "# Save the array as a CSV file named np.csv\n",
    "np.savetxt(\"np.csv\", random_array, delimiter=\",\")\n",
    "\n",
    "# Create a DataFrame from the CSV file\n",
    "df_from_csv = pd.read_csv(\"np.csv\", header=None)\n",
    "\n",
    "# Print the results of the DataFrame\n",
    "print(df_from_csv)\n",
    "\n",
    "# Write the DataFrame to a new CSV file\n",
    "df_from_csv.to_csv(\"new_np.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f825555-bdaf-4185-8eae-f54ada354a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the pickle file: 12239\n",
      "Shape of the retrieved DataFrame: (365, 4)\n",
      "            0         1         2         3\n",
      "0    0.548814  0.715189  0.602763  0.544883\n",
      "1    0.423655  0.645894  0.437587  0.891773\n",
      "2    0.963663  0.383442  0.791725  0.528895\n",
      "3    0.568045  0.925597  0.071036  0.087129\n",
      "4    0.020218  0.832620  0.778157  0.870012\n",
      "..        ...       ...       ...       ...\n",
      "360  0.489685  0.131687  0.397014  0.704402\n",
      "361  0.284886  0.103988  0.907898  0.709051\n",
      "362  0.615276  0.792499  0.835646  0.483459\n",
      "363  0.881188  0.916419  0.271551  0.607545\n",
      "364  0.526584  0.537946  0.937663  0.305189\n",
      "\n",
      "[365 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Tasks 2 & 3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Generate a 365x4 NumPy array with random values\n",
    "np.random.seed(0)\n",
    "random_array_large = np.random.rand(365, 4)\n",
    "\n",
    "# Store the large array in a CSV file and check its size\n",
    "np.savetxt(\"large_np.csv\", random_array_large, delimiter=\",\")\n",
    "\n",
    "# Save the large array in the NumPy binary format (.npy)\n",
    "np.save(\"large_np.npy\", random_array_large)\n",
    "\n",
    "# Load the array from the .npy file\n",
    "loaded_array = np.load(\"large_np.npy\")\n",
    "\n",
    "# Create a DataFrame from this array\n",
    "df_from_npy = pd.DataFrame(loaded_array)\n",
    "\n",
    "# Write the DataFrame to a pickle file\n",
    "df_from_npy.to_pickle(\"df_from_npy.pkl\")\n",
    "\n",
    "# Retrieve the DataFrame from the pickle file\n",
    "retrieved_df = pd.read_pickle(\"df_from_npy.pkl\")\n",
    "\n",
    "# Print the size of the pickle file and the retrieved DataFrame\n",
    "pickle_file_size = os.path.getsize(\"df_from_npy.pkl\")\n",
    "print(\"Size of the pickle file:\", pickle_file_size)\n",
    "print(\"Shape of the retrieved DataFrame:\", retrieved_df.shape)\n",
    "\n",
    "# Create a DataFrame from the large array and write it to an Excel file\n",
    "df_from_npy.to_excel(\"large_data.xlsx\", index=False)\n",
    "\n",
    "# Read the DataFrame back from the Excel file\n",
    "df_from_excel = pd.read_excel(\"large_data.xlsx\")\n",
    "\n",
    "# Print the results of the DataFrame from Excel\n",
    "print(df_from_excel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "990a1531-53f7-4906-9d50-32ffa0260939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original country value:\n",
      "0    Netherlands\n",
      "Name: country, dtype: object\n",
      "\n",
      "Modified DataFrame with new country value:\n",
      "       country dma_code          timezone area_code            ip       asn  \\\n",
      "0  Philippines        0  Europe/Amsterdam         0  46.19.37.108  AS196752   \n",
      "\n",
      "  continent_code           isp  longitude  latitude country_code country_code3  \n",
      "0             EU  Tilaa V.O.F.       5.75      52.5           NL           NLD  \n",
      "\n",
      "Modified JSON string with new country value:\n",
      "[{\"country\":\"Philippines\",\"dma_code\":\"0\",\"timezone\":\"Europe\\/Amsterdam\",\"area_code\":\"0\",\"ip\":\"46.19.37.108\",\"asn\":\"AS196752\",\"continent_code\":\"EU\",\"isp\":\"Tilaa V.O.F.\",\"longitude\":5.75,\"latitude\":52.5,\"country_code\":\"NL\",\"country_code3\":\"NLD\"}]\n",
      "\n",
      "Modified JSON string after changing the country value again:\n",
      "{\"0\":{\"country\":\"Philippines\",\"dma_code\":\"0\",\"timezone\":\"Europe\\/Amsterdam\",\"area_code\":\"0\",\"ip\":\"46.19.37.108\",\"asn\":\"AS196752\",\"continent_code\":\"EU\",\"isp\":\"Tilaa V.O.F.\",\"longitude\":5.75,\"latitude\":52.5,\"country_code\":\"NL\",\"country_code3\":\"NLD\"},\"country\":\"England\"}\n"
     ]
    }
   ],
   "source": [
    "# Tasks 4 & 5\n",
    "import pandas as pd\n",
    "import json\n",
    "from io import StringIO\n",
    "\n",
    "# Given JSON string\n",
    "json_str = '{\"country\":\"Netherlands\",\"dma_code\":\"0\",\"timezone\":\"Europe/Amsterdam\",\"area_code\":\"0\",\"ip\":\"46.19.37.108\",\"asn\":\"AS196752\",\"continent_code\":\"EU\",\"isp\":\"Tilaa V.O.F.\",\"longitude\":5.75,\"latitude\":52.5,\"country_code\":\"NL\",\"country_code3\":\"NLD\"}'\n",
    "\n",
    "# Wrap the JSON string in a StringIO object \n",
    "json_data_io = StringIO(json_str)\n",
    "\n",
    "# Parse JSON string with loads() function to create a dictionary\n",
    "data = json.loads(json_str)\n",
    "\n",
    "# Create a DataFrame from the parsed JSON\n",
    "df = pd.DataFrame([data])\n",
    "\n",
    "# Print the values for the “Country” column\n",
    "print(\"Original country value:\")\n",
    "print(df['country'])\n",
    "\n",
    "# Overwrite the value for Netherlands with 'Philippines'\n",
    "df.at[0, 'country'] = 'Philippines'\n",
    "\n",
    "# Print the modified DataFrame\n",
    "print(\"\\nModified DataFrame with new country value:\")\n",
    "print(df)\n",
    "\n",
    "# Convert the DataFrame to a JSON string\n",
    "json_result = df.to_json(orient='records')\n",
    "\n",
    "# Print the JSON string\n",
    "print(\"\\nModified JSON string with new country value:\")\n",
    "print(json_result)\n",
    "\n",
    "# Wrap the JSON string in a StringIO object before passing it to read_json()\n",
    "json_result_io = StringIO(json_result)\n",
    "\n",
    "# Use the Pandas read_json() function to create a Series from the JSON string\n",
    "series = pd.read_json(json_result_io, typ='series')\n",
    "\n",
    "# Change the country value again \n",
    "series['country'] = 'England'\n",
    "\n",
    "# Convert the Pandas Series to a JSON string\n",
    "json_series_result = series.to_json()\n",
    "\n",
    "# Print the modified JSON string\n",
    "print(\"\\nModified JSON string after changing the country value again:\")\n",
    "print(json_series_result)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c1467d0-e7b0-48be-9792-c04d79e44edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First div: <div class=\"tile\">\n",
      "<h4>Development</h4>\n",
      "     0.10.1 - July 2014<br/>\n",
      "</div>\n",
      "First div class: ['tile']\n",
      "First dfn text: Quare attende, quaeso.\n",
      "Link text: loripsum.net URL: http://loripsum.net/\n",
      "Link text: Poterat autem inpune; URL: http://loripsum.net/\n",
      "Link text: Is es profecto tu. URL: http://loripsum.net/\n",
      "Div 0 contents: ['\\n', <h4>Development</h4>, '\\n     0.10.1 - July 2014', <br/>, '\\n']\n",
      "Div 1 contents: ['\\n', <h4>Official Release</h4>, '\\n     0.10.0 June 2014', <br/>, '\\n']\n",
      "Div 2 contents: ['\\n', <h4>Previous Release</h4>, '\\n     0.09.1 June 2013', <br/>, '\\n']\n",
      "Official Version: Official Release0.10.0 June 2014\n",
      "# elements with class: 3\n",
      "# Title classes: 0\n",
      "# Divs with class containing title: 0\n",
      "Using CSS selector for notitle: []\n",
      "Selecting ordered list first list items: [<li>Cur id non ita fit?</li>, <li>In qua si nihil est praeter rationem, sit in una virtute finis bonorum;</li>]\n",
      "Second list item in ordered list: In qua si nihil est praeter rationem, sit in una virtute finis bonorum;\n",
      "Searching for text string '2014': ['\\n     0.10.1 - July 2014', '\\n     0.10.0 June 2014']\n"
     ]
    }
   ],
   "source": [
    "# Task 6\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# GitHub link to the loremIpsum.html file\n",
    "url = \"https://raw.githubusercontent.com/Rexsophy/Python-Data-Analysis/master/Chapter05/loremIpsum.html\"\n",
    "\n",
    "# Use requests to get the content of the HTML page from the GitHub link\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "    # First div element and its class\n",
    "    first_div = soup.find('div')\n",
    "    print(\"First div:\", first_div)\n",
    "    print(\"First div class:\", first_div.get('class') if first_div else 'No class attribute')\n",
    "\n",
    "    # First dfn text\n",
    "    dfn_text = soup.dfn.text if soup.dfn else 'No dfn tag found'\n",
    "    print(\"First dfn text:\", dfn_text)\n",
    "\n",
    "    # All hyperlinks with their text and URLs\n",
    "    for link in soup.find_all('a'):\n",
    "        print(\"Link text:\", link.string, \"URL:\", link.get('href'))\n",
    "\n",
    "    # Content of all div tags\n",
    "    for i, div in enumerate(soup.find_all('div')):\n",
    "        print(f\"Div {i} contents:\", div.contents)\n",
    "\n",
    "    # Div with id=\"official\" and its specific content\n",
    "    official_div = soup.find(\"div\", id=\"official\")\n",
    "    print(\"Official Version:\", official_div.get_text(strip=True) if official_div else 'No official div')\n",
    "\n",
    "    # Number of div elements with any class attribute\n",
    "    print(\"# elements with class:\", len(soup.find_all(\"div\", class_=True)))\n",
    "\n",
    "    # Number of div elements with class=\"title\"\n",
    "    title_class = soup.find_all(\"div\", class_=\"title\")\n",
    "    print(\"# Title classes:\", len(title_class))\n",
    "\n",
    "    # Number of div elements with class containing \"title\" using regex\n",
    "    divs_with_tile = soup.find_all(\"div\", class_=re.compile(r\"\\btitle\\b\"))\n",
    "    print(\"# Divs with class containing title:\", len(divs_with_tile))\n",
    "\n",
    "    # CSS selector to find divs with class \"notitle\"\n",
    "    print(\"Using CSS selector for notitle:\", soup.select('div.notitle'))\n",
    "\n",
    "    # The first two list items in an ordered list\n",
    "    print(\"Selecting ordered list first list items:\", soup.select(\"ol > li\")[:2])\n",
    "\n",
    "    # The second list item in an ordered list using CSS selector\n",
    "    print(\"Second list item in ordered list:\", soup.select_one(\"ol > li:nth-of-type(2)\").get_text(strip=True))\n",
    "\n",
    "    # Text nodes containing \"2014\"\n",
    "    print(\"Searching for text string '2014':\", soup.find_all(string=re.compile(\"2014\")))\n",
    "else:\n",
    "    print(f\"Failed to fetch HTML content from GitHub. Status code: {response.status_code}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
